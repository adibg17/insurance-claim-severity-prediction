# insurance-claim-severity-prediction
# Insurance Claim Severity Prediction (WIP)

This project aims to predict the severity of insurance claims using PySpark for big data processing, Azure Data Lake for storage, and Tableau for business insights â€” inspired by real-world insurance analytics workflows like those at AXA GBS.

## ğŸš€ Objectives
- Build an end-to-end data pipeline for insurance claim severity prediction
- Use PySpark to preprocess and engineer features on large datasets
- Store data in Azure Data Lake for scalable access
- Train regression models to estimate claim severity
- Visualize business-impact metrics with Tableau dashboards

## ğŸ› ï¸ Tech Stack
- PySpark
- Azure Data Lake (planned integration)
- Tableau
- Python (Pandas, Scikit-learn for baseline)
- Kaggle: [Allstate Claims Severity Dataset](https://www.kaggle.com/competitions/allstate-claims-severity)

## ğŸ“Œ Status
> ğŸ”§ **Work In Progress** â€“ currently working on data cleaning and PySpark integration.

## ğŸ“ˆ Planned Features
- [x] Load & clean dataset
- [ ] PySpark-based feature engineering
- [ ] Model training pipeline
- [ ] Azure storage integration
- [ ] Tableau dashboard for claim trends

## ğŸ“ Folder Overview
- `notebooks/`: Jupyter notebooks with data cleaning and modeling
- `data/`: Sample dataset for local testing
- `tableau/`: Dashboard files (to be added)

---

**Author:** Aditya B.G  
ğŸ“« [adityabg11@gmail.com](mailto:adityabg11@gmail.com)
